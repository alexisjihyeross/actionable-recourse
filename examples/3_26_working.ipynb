{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello! Thank you for checking out our tool.\n",
    "\n",
    "The purpose of this demo is demonstrate some of the basics. In doing so, we will generate a flipset for one individual. In doing so, we'll show:\n",
    "\n",
    "1. How to use the ActionSet interface to specify immutable variables and variables with custom ranges.\n",
    "2. How to use a model to align an ActionSet\n",
    "3. How to use the RecourseBuilder interface to find the feasibility of one person.\n",
    "\n",
    "We'll work using CPLEX. The problem is equivalent for CBC. To install either package, read [here](https://github.com/ustunb/actionable-recourse/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from recourse.builder import RecourseBuilder\n",
    "from recourse.builder import ActionSet\n",
    "from recourse.flipset import Flipset\n",
    "\n",
    "data_dir = \"../data/2_1_experiment_1/\"\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def ohe(data, categorical_names, encoder, columns = []):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        df_data = data.copy()\n",
    "    else:\n",
    "        if len(columns) == 0:\n",
    "            raise ValueError('Need to supply columns to make a pandas dataframe')\n",
    "        df_data = pd.DataFrame(data, columns = columns)        \n",
    "    transformed = encoder.transform(df_data[categorical_names])\n",
    "    df_transformed = (pd.DataFrame(transformed, columns = encoder.get_feature_names(input_features = categorical_names)))\n",
    "    return pd.concat([df_data.reset_index(drop = True), df_transformed], axis=1).drop(categorical_names, axis=1)\n",
    "\n",
    "def un_ohe(ohe_data, categorical_names, encoder, columns):\n",
    "    ohe_categorical_columns = encoder.get_feature_names(input_features = categorical_names)\n",
    "    if len(columns) == 0:\n",
    "            raise ValueError('Need to supply columns to make a pandas dataframe')\n",
    "    if isinstance(ohe_data, pd.DataFrame):\n",
    "        df_ohe_data = ohe_data.copy()\n",
    "    else:\n",
    "        df_ohe_data = pd.DataFrame(ohe_data, columns = columns)        \n",
    "    untransformed = encoder.inverse_transform(df_ohe_data[ohe_categorical_columns])\n",
    "    untransformed_df = pd.DataFrame(untransformed, columns = categorical_names)\n",
    "    to_return = pd.concat([df_ohe_data.reset_index(drop = True), untransformed_df], axis=1).drop(ohe_categorical_columns, axis=1)\n",
    "    return to_return[columns]\n",
    "\n",
    "# #NEED TO FINISH\n",
    "# def ohe_coefficients(w, x, columns, enc, categorical_names, ohe_categorical_columns):\n",
    "#     new_coefficients = []\n",
    "#     categories_mapping = {}\n",
    "#     ohe_columns = ohe(x, categorical_names, enc, columns = columns).columns\n",
    "    \n",
    "#     for feat_idx, feat in enumerate(categorical_names):\n",
    "#         categories_mapping[feat] = compas_enc.categories_[feat_idx]\n",
    "    \n",
    "#     for feat in ohe_columns:\n",
    "#         if \"_\"\n",
    "#         new_coefficients.append\n",
    "#     display(compas_enc.categories_)\n",
    "\n",
    "\n",
    "def get_label_encoders(categorical_names, data):\n",
    "    return_data = data.copy()\n",
    "    categorical_encoders = {}\n",
    "\n",
    "    for cf in categorical_names:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(data[cf])\n",
    "        return_data[cf] = le.transform(return_data[cf])\n",
    "        categorical_encoders[cf] = le\n",
    "    \n",
    "    return categorical_encoders, return_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPAS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>length_of_stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6902</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6903</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6904</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6905</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6907 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  two_year_recid  c_charge_degree  race  sex  priors_count  \\\n",
       "0      69               0                0     5    1             0   \n",
       "1      34               1                0     0    1             0   \n",
       "2      24               1                0     0    1             4   \n",
       "3      44               0                1     5    1             0   \n",
       "4      41               1                0     2    1            14   \n",
       "...   ...             ...              ...   ...  ...           ...   \n",
       "6902   23               0                0     0    1             0   \n",
       "6903   23               0                0     0    1             0   \n",
       "6904   57               0                0     5    1             0   \n",
       "6905   33               0                1     0    0             3   \n",
       "6906   23               1                0     3    0             2   \n",
       "\n",
       "      length_of_stay  \n",
       "0                0.0  \n",
       "1               10.0  \n",
       "2                1.0  \n",
       "3                1.0  \n",
       "4                6.0  \n",
       "...              ...  \n",
       "6902             1.0  \n",
       "6903             1.0  \n",
       "6904             1.0  \n",
       "6905             1.0  \n",
       "6906             1.0  \n",
       "\n",
       "[6907 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>two_year_recid_0</th>\n",
       "      <th>two_year_recid_1</th>\n",
       "      <th>c_charge_degree_0</th>\n",
       "      <th>c_charge_degree_1</th>\n",
       "      <th>race_0</th>\n",
       "      <th>race_1</th>\n",
       "      <th>race_2</th>\n",
       "      <th>race_3</th>\n",
       "      <th>race_4</th>\n",
       "      <th>race_5</th>\n",
       "      <th>sex_0</th>\n",
       "      <th>sex_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6902</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6903</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6904</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6905</th>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6907 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  priors_count  length_of_stay  two_year_recid_0  two_year_recid_1  \\\n",
       "0      69             0             0.0               1.0               0.0   \n",
       "1      34             0            10.0               0.0               1.0   \n",
       "2      24             4             1.0               0.0               1.0   \n",
       "3      44             0             1.0               1.0               0.0   \n",
       "4      41            14             6.0               0.0               1.0   \n",
       "...   ...           ...             ...               ...               ...   \n",
       "6902   23             0             1.0               1.0               0.0   \n",
       "6903   23             0             1.0               1.0               0.0   \n",
       "6904   57             0             1.0               1.0               0.0   \n",
       "6905   33             3             1.0               1.0               0.0   \n",
       "6906   23             2             1.0               0.0               1.0   \n",
       "\n",
       "      c_charge_degree_0  c_charge_degree_1  race_0  race_1  race_2  race_3  \\\n",
       "0                   1.0                0.0     0.0     0.0     0.0     0.0   \n",
       "1                   1.0                0.0     1.0     0.0     0.0     0.0   \n",
       "2                   1.0                0.0     1.0     0.0     0.0     0.0   \n",
       "3                   0.0                1.0     0.0     0.0     0.0     0.0   \n",
       "4                   1.0                0.0     0.0     0.0     1.0     0.0   \n",
       "...                 ...                ...     ...     ...     ...     ...   \n",
       "6902                1.0                0.0     1.0     0.0     0.0     0.0   \n",
       "6903                1.0                0.0     1.0     0.0     0.0     0.0   \n",
       "6904                1.0                0.0     0.0     0.0     0.0     0.0   \n",
       "6905                0.0                1.0     1.0     0.0     0.0     0.0   \n",
       "6906                1.0                0.0     0.0     0.0     0.0     1.0   \n",
       "\n",
       "      race_4  race_5  sex_0  sex_1  \n",
       "0        0.0     1.0    0.0    1.0  \n",
       "1        0.0     0.0    0.0    1.0  \n",
       "2        0.0     0.0    0.0    1.0  \n",
       "3        0.0     1.0    0.0    1.0  \n",
       "4        0.0     0.0    0.0    1.0  \n",
       "...      ...     ...    ...    ...  \n",
       "6902     0.0     0.0    0.0    1.0  \n",
       "6903     0.0     0.0    0.0    1.0  \n",
       "6904     0.0     1.0    0.0    1.0  \n",
       "6905     0.0     0.0    1.0    0.0  \n",
       "6906     0.0     0.0    1.0    0.0  \n",
       "\n",
       "[6907 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>length_of_stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6902</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6903</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6904</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6905</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6907 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  two_year_recid  c_charge_degree  race  sex  priors_count  \\\n",
       "0      69               0                0     5    1             0   \n",
       "1      34               1                0     0    1             0   \n",
       "2      24               1                0     0    1             4   \n",
       "3      44               0                1     5    1             0   \n",
       "4      41               1                0     2    1            14   \n",
       "...   ...             ...              ...   ...  ...           ...   \n",
       "6902   23               0                0     0    1             0   \n",
       "6903   23               0                0     0    1             0   \n",
       "6904   57               0                0     5    1             0   \n",
       "6905   33               0                1     0    0             3   \n",
       "6906   23               1                0     3    0             2   \n",
       "\n",
       "      length_of_stay  \n",
       "0                0.0  \n",
       "1               10.0  \n",
       "2                1.0  \n",
       "3                1.0  \n",
       "4                6.0  \n",
       "...              ...  \n",
       "6902             1.0  \n",
       "6903             1.0  \n",
       "6904             1.0  \n",
       "6905             1.0  \n",
       "6906             1.0  \n",
       "\n",
       "[6907 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_name = \"compas-scores-two-years\"\n",
    "data_file = os.path.join(data_dir, '%s.csv' % data_name)\n",
    "## load and process data\n",
    "compas_df = pd.read_csv(data_file).reset_index(drop=True)\n",
    "\n",
    "# filter according to https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb and https://github.com/dylan-slack/Fooling-LIME-SHAP/blob/e763fdea8242f4f3a5955951161c69f573db624d/get_data.py#L5\n",
    "# compas_df = compas_df.loc[(compas_df['days_b_screening_arrest'] <= 30) & \\\n",
    "#                           (compas_df['days_b_screening_arrest'] >= -30) & \\\n",
    "#                           (compas_df['is_recid'] != -1) & \\\n",
    "#                           (compas_df['c_charge_degree'] != \"O\") & \\\n",
    "#                           (compas_df['score_text'] != \"NA\")]\n",
    "\n",
    "# cols_with_missing_values = []\n",
    "# for col in compas_df.columns:\n",
    "#     if len(np.where(compas_df[col].values == '?')[0]) >= 1 or compas_df[col].isnull().values.any():\n",
    "#         cols_with_missing_values.append(col)    \n",
    "# compas_df = compas_df.drop(cols_with_missing_values, axis=1)\n",
    "\n",
    "compas_df['length_of_stay'] = (pd.to_datetime(compas_df['c_jail_out']) - pd.to_datetime(compas_df['c_jail_in'])).dt.days\n",
    "\n",
    "compas_df = compas_df[['age', 'two_year_recid','c_charge_degree', 'race', 'sex', 'priors_count', 'length_of_stay', 'score_text']]\n",
    "compas_df = compas_df.dropna()\n",
    "\n",
    "compas_X = compas_df.drop('score_text', axis=1)\n",
    "\n",
    "# compas_df = (compas_df\n",
    "#              .drop(['id', 'name', 'first', 'last', 'dob', 'compas_screening_date', \\\n",
    "#                     'screening_date', 'v_type_of_assessment', 'type_of_assessment', 'v_screening_date'], axis=1)\n",
    "#             )\n",
    "\n",
    "# compas_df = pd.get_dummies(compas_df, columns=['sex']).drop(['sex_Female'], axis=1)\n",
    "# compas_df = pd.get_dummies(compas_df, columns=['age_cat'])\n",
    "# compas_df = pd.get_dummies(compas_df, columns=['score_text'])\n",
    "# compas_df = pd.get_dummies(compas_df, columns=['v_score_text'])\n",
    "# compas_df = pd.get_dummies(compas_df, columns=['c_charge_degree']).drop(['c_charge_degree_F'], axis=1)\n",
    "\n",
    "compas_y = pd.Series(np.array([-1 if score == 'High' else 1 for score in compas_df['score_text']]))\n",
    "# compas_X = compas_df.drop('score_text', axis=1)\n",
    "\n",
    "\n",
    "# CATEGORICAL FEATURES\n",
    "compas_categorical_features = [1, 2, 3, 4]\n",
    "\n",
    "compas_X = compas_X.reset_index(drop = True)\n",
    "compas_y = compas_y.reset_index(drop = True)\n",
    "\n",
    "columns = compas_X.columns\n",
    "compas_categorical_names = [columns[i] for i in compas_categorical_features] \n",
    "\n",
    "\n",
    "# ENCODE NUMERICALLY\n",
    "compas_label_encoders, compas_X = (get_label_encoders(compas_categorical_names, compas_X))\n",
    "\n",
    "# CREATE ENCODER\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "compas_enc = OneHotEncoder(sparse = False, handle_unknown='error')\n",
    "compas_enc.fit(compas_X[compas_categorical_names])\n",
    "\n",
    "display(compas_X)\n",
    "display(ohe(compas_X, compas_categorical_names, compas_enc))\n",
    "display(un_ohe(ohe(compas_X, compas_categorical_names, compas_enc), compas_categorical_names, compas_enc, columns = compas_X.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Credit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'german_processed'\n",
    "data_file = os.path.join(data_dir, '%s.csv' % data_name)\n",
    "## load and process data\n",
    "german_df = pd.read_csv(data_file).reset_index(drop=True)\n",
    "\n",
    "german_df = (german_df\n",
    "             .assign(isMale=lambda df: (df['Gender']=='Male').astype(int))\n",
    "             .drop(['PurposeOfLoan', 'Gender', 'OtherLoansAtStore'], axis=1)\n",
    "            )\n",
    "\n",
    "german_y = german_df['GoodCustomer']\n",
    "german_X = german_df.drop('GoodCustomer', axis=1)\n",
    "\n",
    "german_categorical_features = [0, 1, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
    "columns = german_X.columns\n",
    "german_categorical_names = [columns[i] for i in german_categorical_features] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "36    898\n",
      "31    888\n",
      "34    886\n",
      "23    877\n",
      "35    876\n",
      "     ... \n",
      "83      6\n",
      "85      3\n",
      "88      3\n",
      "87      1\n",
      "86      1\n",
      "Name: age, Length: 73, dtype: int64\n",
      "education-num\n",
      "9     10501\n",
      "10     7291\n",
      "13     5354\n",
      "14     1723\n",
      "11     1382\n",
      "7      1175\n",
      "12     1067\n",
      "6       933\n",
      "4       646\n",
      "15      576\n",
      "5       514\n",
      "8       433\n",
      "16      413\n",
      "3       333\n",
      "2       168\n",
      "1        51\n",
      "Name: education-num, dtype: int64\n",
      "capital-gain\n",
      "0        29849\n",
      "15024      347\n",
      "7688       284\n",
      "7298       246\n",
      "99999      159\n",
      "         ...  \n",
      "4931         1\n",
      "1455         1\n",
      "6097         1\n",
      "22040        1\n",
      "1111         1\n",
      "Name: capital-gain, Length: 119, dtype: int64\n",
      "capital-loss\n",
      "0       31041\n",
      "1902      202\n",
      "1977      168\n",
      "1887      159\n",
      "1848       51\n",
      "        ...  \n",
      "1411        1\n",
      "1539        1\n",
      "2472        1\n",
      "1944        1\n",
      "2201        1\n",
      "Name: capital-loss, Length: 92, dtype: int64\n",
      "hours-per-week\n",
      "40    15216\n",
      "50     2819\n",
      "45     1824\n",
      "60     1475\n",
      "35     1297\n",
      "      ...  \n",
      "92        1\n",
      "94        1\n",
      "87        1\n",
      "74        1\n",
      "82        1\n",
      "Name: hours-per-week, Length: 94, dtype: int64\n",
      "Married\n",
      "0    17143\n",
      "1    15417\n",
      "Name: Married, dtype: int64\n",
      "Widowed\n",
      "0    31567\n",
      "1      993\n",
      "Name: Widowed, dtype: int64\n",
      "NeverMarried\n",
      "0    21878\n",
      "1    10682\n",
      "Name: NeverMarried, dtype: int64\n",
      "workclass_gov\n",
      "0    28210\n",
      "1     4350\n",
      "Name: workclass_gov, dtype: int64\n",
      "workclass_private\n",
      "1    22696\n",
      "0     9864\n",
      "Name: workclass_private, dtype: int64\n",
      "workclass_self-emp\n",
      "0    28903\n",
      "1     3657\n",
      "Name: workclass_self-emp, dtype: int64\n",
      "White\n",
      "1    27815\n",
      "0     4745\n",
      "Name: White, dtype: int64\n",
      "sex_Male\n",
      "1    21789\n",
      "0    10771\n",
      "Name: sex_Male, dtype: int64\n",
      "Index(['age', 'education-num', 'capital-gain', 'capital-loss',\n",
      "       'hours-per-week', 'Married', 'Widowed', 'NeverMarried', 'workclass_gov',\n",
      "       'workclass_private', 'workclass_self-emp', 'White', 'sex_Male'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_name = \"adult\"\n",
    "data_file = os.path.join(data_dir, '%s.csv' % data_name)\n",
    "## load and process data\n",
    "adult_df = pd.read_csv(data_file).reset_index(drop=True)\n",
    "adult_df.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex',\\\n",
    "                                          'capital-gain','capital-loss','hours-per-week','native-country','label']\n",
    "\n",
    "cols_with_missing_values = []\n",
    "for col in adult_df.columns:\n",
    "    if len(np.where(adult_df[col].values == '?')[0]) >= 1 or adult_df[col].isnull().values.any():\n",
    "        cols_with_missing_values.append(col)    \n",
    "\n",
    "adult_df = adult_df.drop(cols_with_missing_values, axis=1)\n",
    "\n",
    "adult_df['Married'] = adult_df.apply(lambda row: 1 if 'Married' in row['marital-status'] else 0, axis=1)\n",
    "adult_df['Widowed'] = adult_df.apply(lambda row: 1 if 'Widowed' in row['marital-status'] else 0, axis=1)\n",
    "adult_df['NeverMarried'] = adult_df.apply(lambda row: 1 if 'Never-married' in row['marital-status'] else 0, axis=1)\n",
    "\n",
    "adult_df['workclass_gov'] = adult_df.apply(lambda row: 1 if 'gov' in row['workclass'] else 0, axis=1)\n",
    "adult_df['workclass_private'] = adult_df.apply(lambda row: 1 if 'Private' in row['workclass'] else 0, axis=1)\n",
    "adult_df['workclass_self-emp'] = adult_df.apply(lambda row: 1 if 'Self-emp' in row['workclass'] else 0, axis=1)\n",
    "# adult_df['workclass_never-worked'] = adult_df.apply(lambda row: 1 if 'Never-worked' in row['workclass'] else 0, axis=1)\n",
    "\n",
    "adult_df['White'] = adult_df.apply(lambda row: 1 if 'White' in row['race'] else 0, axis=1)\n",
    "\n",
    "# adult_df = pd.get_dummies(adult_df, columns=['race'])\n",
    "adult_df = pd.get_dummies(adult_df, columns=['sex'])\n",
    "\n",
    "adult_df = adult_df.drop(['education', 'occupation', 'native-country', \\\n",
    "                          'relationship'], axis=1)\n",
    "adult_df = adult_df.drop(['sex_ Female', 'race'], axis=1)\n",
    "adult_df = adult_df.drop(['marital-status', 'workclass', 'fnlwgt'], axis=1)\n",
    "\n",
    "adult_df.columns = adult_df.columns.str.replace(' ', '')\n",
    "\n",
    "adult_X = adult_df.drop('label', axis=1)\n",
    "adult_y = adult_df['label'].replace(' <=50K', -1)\n",
    "adult_y = adult_y.replace(' >50K', 1)\n",
    "\n",
    "for col in adult_X.columns:\n",
    "    print(col)\n",
    "    print(adult_X[col].value_counts())\n",
    "\n",
    "adult_categorical_features = [5, 6, 7, 8, 9, 10, 11, 12]\n",
    "columns = adult_X.columns\n",
    "print(columns)\n",
    "adult_categorical_names = [columns[i] for i in adult_categorical_features] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the data not ohe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need the data for recourse and lime to be NOT one-hot-encoded and to be numerical\n",
    "#need the data for the classifier to be one hot encoded\n",
    "\n",
    "# german_df['YearsAtCurrentJob_lt_1'] = german_df['YearsAtCurrentJob_lt_1'].replace(1, 'lt_1')\n",
    "# german_df['YearsAtCurrentJob'] = german_df['YearsAtCurrentJob_lt_1']\n",
    "# german_df['YearsAtCurrentJob_geq_4'] = german_df['YearsAtCurrentJob_geq_4'].replace(1, 'geq_4')\n",
    "# german_df['YearsAtCurrentJob'] = german_df.apply(lambda row: 'geq_4' if row['YearsAtCurrentJob_geq_4'] == 'geq_4' else row['YearsAtCurrentJob'], axis=1)\n",
    "# german_df['YearsAtCurrentJob'] = german_df['YearsAtCurrentJob_lt_1'].replace(0, 'bet_1_4')\n",
    "# german_df = german_df.drop(['YearsAtCurrentJob_lt_1', 'YearsAtCurrentJob_geq_4'], axis=1)\n",
    "\n",
    "# german_df['CheckingAccountBalance_geq_0'] = german_df['CheckingAccountBalance_geq_0'].replace(1, 'geq_0')\n",
    "# german_df['CheckingAccountBalance_geq_200'] = german_df['CheckingAccountBalance_geq_200'].replace(1, 'geq_200')\n",
    "# german_df['CheckingAccountBalance'] = german_df['CheckingAccountBalance_geq_0']\n",
    "# german_df['CheckingAccountBalance'] = german_df.apply(lambda row: 'geq_200' if row['CheckingAccountBalance_geq_200'] == 'geq_200' else row['CheckingAccountBalance'], axis=1)\n",
    "# german_df['CheckingAccountBalance'] = german_df['CheckingAccountBalance'].replace('geq_0', '0_200')\n",
    "# german_df = german_df.drop(['CheckingAccountBalance_geq_0', 'CheckingAccountBalance_geq_200'], axis=1)\n",
    "\n",
    "# german_df['SavingsAccountBalance_geq_100'] = german_df['SavingsAccountBalance_geq_100'].replace(1, '100_500')\n",
    "# german_df['SavingsAccountBalance_geq_500'] = german_df['SavingsAccountBalance_geq_500'].replace(1, 'geq_500')\n",
    "# german_df['SavingsAccountBalance'] = german_df['SavingsAccountBalance_geq_100']\n",
    "# german_df['SavingsAccountBalance'] = german_df.apply(lambda row: 'geq_500' if row['SavingsAccountBalance_geq_500'] == 'geq_500' else row['SavingsAccountBalance'], axis=1)\n",
    "# german_df['SavingsAccountBalance'] = german_df['SavingsAccountBalance'].replace('0', 'lt_100')\n",
    "# german_df = german_df.drop(['SavingsAccountBalance_geq_100', 'SavingsAccountBalance_geq_500'], axis=1)\n",
    "# display(german_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# display(X)\n",
    "# display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk = np.random.rand(len(X)) < 0.8\n",
    "# train = X[msk]\n",
    "# test = X[~msk]\n",
    "\n",
    "# train_y = y[msk]\n",
    "# test_y = y[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, no immutable features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great, now let's get into the meat of it. Let's train up a model as see what recourse exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Recourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's score everyone using our model. Now, let's say that we will give loans to anyone with a greater than a $80\\%$ chance of paying it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(\"../..\")\n",
    "from mlinsights.mlinsights.mlmodel import PiecewiseRegressor\n",
    "\n",
    "\n",
    "# start by randomly picking an action for each feature\n",
    "def sample_with_actions(instance, actions, num_samples, ordered_feature_names):\n",
    "    num_features = len(ordered_feature_names)\n",
    "    sampled_data = np.zeros((num_samples, num_features))\n",
    "    sampled_data[0, :] = instance\n",
    "    \n",
    "    len_actions = [len(actions[feat]) for feat in ordered_feature_names]\n",
    "    ordered_actions = [actions[feat] for feat in ordered_feature_names]\n",
    "    \n",
    "#     print(\"instance: \", instance)\n",
    "#     print(\"actions: \", ordered_actions)\n",
    "#     print(\"len_actions; \", len_actions)\n",
    "#     print(\"ordered_actions: \", ordered_actions)\n",
    "    \n",
    "    # max number of actions\n",
    "    max_actions = len(actions[max(actions, key=lambda feat:len(actions[feat]))])\n",
    "            \n",
    "#     print(len_actions)    \n",
    "        \n",
    "    for s in range(1, num_samples):\n",
    "        sampled_actions = [ordered_actions[i][np.random.choice(x)] for i, x in enumerate(len_actions)]\n",
    "#         print(\"sampled_actions: \", sampled_actions)\n",
    "        sampled_data[s, :] = instance + sampled_actions\n",
    "#         print(\"sampled_actions: \", sampled_actions)\n",
    "        \n",
    "    return sampled_data\n",
    "\n",
    "def convert_binary_categorical_coefficients(exp_list):\n",
    "    cleaned_exp_dict = {}\n",
    "    for (feat, coeff) in exp_list:\n",
    "        if \"=\" in feat:\n",
    "            original_feat, val = feat.split(\"=\")\n",
    "            int_val = int(val)\n",
    "            if int_val == 1:\n",
    "                cleaned_exp_dict[original_feat] = coeff\n",
    "            else:\n",
    "                cleaned_exp_dict[original_feat] = -1 * coeff\n",
    "        else:\n",
    "            cleaned_exp_dict[feat] = coeff\n",
    "    return cleaned_exp_dict\n",
    "\n",
    "# scaled_X = (X - explainer.scaler.mean_) / explainer.scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyutilib.common import ApplicationError\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(\"../..\")\n",
    "    \n",
    "# import MAPLE.MAPLE\n",
    "from MAPLE.Code.Misc import load_normalize_data, unpack_coefs\n",
    "from MAPLE.Code.MAPLE import MAPLE\n",
    "\n",
    "def get_nonzero_actions(feature_names, action):\n",
    "    action_dict = {}\n",
    "    for feat_idx, feat_name in enumerate(feature_names):\n",
    "        action_for_feat = action[feat_idx]\n",
    "        if action_for_feat != 0:\n",
    "            action_dict[feat_name] = action_for_feat\n",
    "    return action_dict\n",
    "\n",
    "# # X_train is unnormalized\n",
    "# def find_testing_interventions(x, model, X_train, binary_categorical_features, num_divisions = 100):\n",
    "#     interventions = [[]] * len(X_train.columns)\n",
    "#     original_pred = model\n",
    "#     for feat_idx, feat in X_train.columns:\n",
    "#         if feat_idx not in binary_categorical_features:\n",
    "#             feat_min = X_train[feat].min()\n",
    "#             feat_max = X_train[feat].max()\n",
    "#             increment = (feat_max - feat_min) / 100\n",
    "#         else:\n",
    "#             x[feat_idx] = \n",
    "        \n",
    "# data is array-like of shape (n_samples, n_features)\n",
    "def get_pred_function(model, categorical_names, enc, columns):\n",
    "    def new_predict_proba(data):\n",
    "        transformed_data = ohe(data, categorical_names, enc, columns = columns)\n",
    "        return model.predict_proba(transformed_data)\n",
    "    return new_predict_proba        \n",
    "    \n",
    "def get_lime_sampled(lime_explainer, x, new_predict_proba, num_features, num_samples, columns, true_index):\n",
    "    exp = lime_explainer.explain_instance(x, new_predict_proba, num_features = num_features, num_samples = num_samples)\n",
    "    return exp.inverse, exp.scaled_data, exp.all_sampled_preds, exp.weights, lime_explainer.scaler.mean_, lime_explainer.scaler.scale_\n",
    "    \n",
    "def get_lime_coefficients(lime_explainer, x, new_predict_proba, num_features, num_samples, columns, true_index):\n",
    "    exp = lime_explainer.explain_instance(x, new_predict_proba, num_features = num_features, num_samples = num_samples)\n",
    "    inverse_data = (pd.DataFrame(exp.inverse, columns = columns))\n",
    "#     display(inverse_data)\n",
    "#     display(pd.DataFrame([x], columns = columns))\n",
    "#     display(inverse_data['race'].value_counts())\n",
    "#     display(inverse_data['sex'].value_counts())\n",
    "    local_pred = exp.local_pred\n",
    "\n",
    "    coefficients = [None] * num_features\n",
    "\n",
    "    local_exp = exp.local_exp[true_index]\n",
    "        \n",
    "    for (feat, coef) in local_exp:\n",
    "        coefficients[feat] = coef\n",
    "    \n",
    "#     cleaned_exp_dict = convert_binary_categorical_coefficients(exp.as_list())\n",
    "    \n",
    "#     for j, col in enumerate(columns):\n",
    "#         coefficients[j] = cleaned_exp_dict[col]\n",
    "\n",
    "        \n",
    "    print(\"exp.local_exp:\", local_exp)\n",
    "        \n",
    "    intercept = exp.intercept[true_index]\n",
    "\n",
    "    x_shift = np.array(lime_explainer.scaler.mean_)\n",
    "    x_scale = np.array(lime_explainer.scaler.scale_)\n",
    "    w = coefficients / x_scale\n",
    "    b = intercept - np.sum(w * x_shift) - 0.5 # subtract 0.5 bc using probs as labels\n",
    "    \n",
    "    discrete_yss = (exp.yss[:, true_index] > 0.5).astype(int)\n",
    "    discrete_sampled_preds = (exp.all_sampled_preds > 0.5).astype(int)\n",
    "    \n",
    "    num_pos_yss = (np.count_nonzero(discrete_yss == 1))\n",
    "    num_neg_yss = (np.count_nonzero(discrete_yss == 0))\n",
    "    \n",
    "    num_accurate_preds = np.count_nonzero(discrete_yss == discrete_sampled_preds)\n",
    "    accuracy_sampled = num_accurate_preds/len(discrete_yss)\n",
    "    \n",
    "    return w, b, local_pred, accuracy_sampled\n",
    "\n",
    "def get_maple_coefficients(maple_explainer, x, mean, std, lime_sampled = [], model_preds_sampled = [], use_distance_weights = True):\n",
    "    if lime_sampled != []:\n",
    "        e_maple = maple_explainer.explain(x, lime_sampled = lime_sampled, model_preds_sampled = model_preds_sampled, use_distance_weights = use_distance_weights)\n",
    "    else:\n",
    "        e_maple = maple_explainer.explain(x, use_distance_weights = use_distance_weights)\n",
    "        \n",
    "    coefs_maple = e_maple[\"coefs\"][1:]\n",
    "    intercept_maple = e_maple[\"coefs\"][0]\n",
    "    \n",
    "    \n",
    "    w = coefs_maple / std\n",
    "    b = intercept_maple - np.sum(w * mean) - 0.5 # subtract 0.5 bc using probs as labels\n",
    "        \n",
    "    num_pos_yss = (np.count_nonzero(e_maple['selected_sampled_yss'] == 1))\n",
    "    num_neg_yss = (np.count_nonzero(e_maple['selected_sampled_yss'] == 0))\n",
    "    \n",
    "    num_accurate_preds = np.count_nonzero(e_maple['selected_sampled_yss'] == e_maple['selected_sampled_preds'])   \n",
    "    accuracy_sampled = num_accurate_preds/len(e_maple['selected_sampled_preds'])\n",
    "    local_pred = e_maple['pred']    \n",
    "    \n",
    "#     print(e_maple['weights'])\n",
    "#     print(len(np.nonzero(e_maple['weights'])[0]))\n",
    "#     print(e_maple['weights'][np.nonzero(e_maple['weights'])[0]])\n",
    "    \n",
    "    return w, b, local_pred, accuracy_sampled\n",
    "\n",
    "def get_piecewise_coefficients_with_maple(maple_explainer, x, lime_sampled = [], model_preds_sampled = [], use_distance_weights = False):\n",
    "    e_maple = maple_explainer.explain(x, lime_sampled = lime_sampled, model_preds_sampled = model_preds_sampled, use_distance_weights = use_distance_weights)\n",
    "    maple_weights = e_maple['weights']\n",
    "    \n",
    "    model = PiecewiseRegressor(verbose=False,\n",
    "                           binner=DecisionTreeRegressor(min_samples_leaf=2500))\n",
    "    model.fit(lime_sampled, model_preds_sampled, sample_weight=maple_weights)\n",
    "    \n",
    "    estimators = model.estimators_\n",
    "    \n",
    "    return estimators    \n",
    "\n",
    "def get_piecewise_coefficients(x, lime_sampled, model_preds_sampled, lime_sampled_weights):\n",
    "    model = PiecewiseRegressor(verbose=False,\n",
    "                           binner=DecisionTreeRegressor(min_samples_leaf=2500))\n",
    "    model.fit(lime_sampled, model_preds_sampled, sample_weight=lime_sampled_weights)\n",
    "    \n",
    "#     sampled_preds = model.predict()\n",
    "    \n",
    "#     accuracy_sampled = \n",
    "    \n",
    "    estimators = model.estimators_\n",
    "    \n",
    "    return estimators\n",
    "\n",
    "# def get_ohe_coefficients(w):\n",
    "#     df_w = pd.DataFrame(w, columns = columns)        \n",
    "#     transformed = encoder.transform(df_data[categorical_names])\n",
    "#     df_transformed = (pd.DataFrame(transformed, columns = encoder.get_feature_names(input_features = categorical_names)))\n",
    "#     return pd.concat([df_data.reset_index(drop = True), df_transformed], axis=1).drop(categorical_names, axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "def get_recourse(x, action_set, w, b):\n",
    "    action_set.align(coefficients=w)\n",
    "    fb = Flipset(x = x, action_set = action_set, coefficients = w, intercept = b)\n",
    "    \n",
    "    try:\n",
    "        print(\"populating...\")\n",
    "        fb = fb.populate(enumeration_type = 'distinct_subsets', total_items = 20)\n",
    "        actions = fb._builder.actions\n",
    "\n",
    "        error = False\n",
    "\n",
    "        returned_actions = [result['actions'] for result in fb.items]\n",
    "\n",
    "    except (ValueError, ApplicationError, AssertionError) as e:\n",
    "        print(\"excepting...\")\n",
    "        print(e)\n",
    "#         print(\"coeffs from error: \", w)\n",
    "        error = True\n",
    "\n",
    "        returned_actions = []\n",
    "    \n",
    "\n",
    "    return returned_actions, error\n",
    "\n",
    "# assumes data is properly formatted\n",
    "def calculate_recourse_accuracy(model, data, enc, categorical_features, categorical_names, file_name, \\\n",
    "    num_samples = 5000, kernel_width = 1, explanation_type = 'lime', lime_sample_around_instance = None, \\\n",
    "    use_lime_sampled_maple = None, maple_use_distance_weights = None, instances_subset = None):\n",
    "    \n",
    "    instances_with_recourses = []\n",
    "    \n",
    "    use_lime_inverse = True\n",
    "    \n",
    "    with open(file_name, \"a\") as f:   \n",
    "        \n",
    "        X_train = data['X_train']\n",
    "        y_train = data['y_train']\n",
    "\n",
    "        X_val = data['X_val']\n",
    "        y_val = data['y_val']\n",
    "\n",
    "        X_test = data['X_test']\n",
    "        y_test = data['y_test']\n",
    "        \n",
    "        ohe_X_train = ohe(X_train, categorical_names, enc)\n",
    "        ohe_X_val = ohe(X_val, categorical_names, enc)\n",
    "        ohe_X_test = ohe(X_test, categorical_names, enc)\n",
    "\n",
    "        print(\"\\n\\nTRAIN LABEL SPLIT: \", file=f)\n",
    "        print(y_train.value_counts(), file=f)\n",
    "\n",
    "        print(\"validation score: \", model.score(ohe_X_val, y_val), file=f)\n",
    "\n",
    "        new_predict_proba = get_pred_function(model, categorical_names, enc, X_train.columns)\n",
    "        \n",
    "        if instances_subset != None:\n",
    "            calculate_subset_accuracy = True\n",
    "            subset_total_recourses = 0\n",
    "            subset_total_actual_recourses = 0\n",
    "        else:\n",
    "            instances_subset = []\n",
    "            calculate_subset_accuracy = False\n",
    "        \n",
    "        classes = model.classes_\n",
    "        true_index = list(classes).index(1)\n",
    "        \n",
    "        scores = pd.Series(new_predict_proba(X_test)[:, true_index])\n",
    "        discrete_scores = pd.Series(model.predict(ohe_X_test))\n",
    "\n",
    "        total_recourses = 0\n",
    "        total_actual_recourses = 0\n",
    "        \n",
    "        total_instances_with_recourses = 0\n",
    "\n",
    "        num_actiongrid_regressor_agree = 0\n",
    "        num_lime_agree = 0\n",
    "        num_sampled_total = 0\n",
    "\n",
    "        print(\"NUM SAMPLES: \", num_samples, file=f)\n",
    "        print(\"KERNEL WIDTH: \", kernel_width, file=f)\n",
    "        print(\"EXPLANATION TYPE: \", explanation_type, file=f)\n",
    "        if lime_sample_around_instance != None:\n",
    "            print(\"LIME SAMPLE AROUND INSTANCES: \", lime_sample_around_instance, file=f)\n",
    "        if use_lime_sampled_maple != None:\n",
    "            print(\"USE SAMPLED LIME FOR MAPLE: \", use_lime_sampled_maple, file=f)\n",
    "        if maple_use_distance_weights != None:\n",
    "            print(\"USE DISTANCE WEIGHTS FOR MAPLE: \", maple_use_distance_weights, file=f)\n",
    "            \n",
    "        print(\"num unique preds: \", np.unique(discrete_scores, axis=0).shape[0])\n",
    "\n",
    "\n",
    "        print(\"TEST LABEL SPLIT: \", file=f)\n",
    "\n",
    "        print(discrete_scores.value_counts())\n",
    "\n",
    "        # class_names have to be ordered according to what the classifier is using\n",
    "        lime_explainer = lime_tabular.LimeTabularExplainer(X_train.values, categorical_features=categorical_features, \n",
    "                                                           categorical_names=categorical_names, \\\n",
    "                                                           feature_names=X_train.columns, class_names=classes, \\\n",
    "                                                           discretize_continuous=False, kernel_width = kernel_width, \\\n",
    "                                                           sample_around_instance = lime_sample_around_instance)\n",
    "\n",
    "        train_stddev = X_train[X_train.columns[:]].std()\n",
    "        train_mean = X_train[X_train.columns[:]].mean()\n",
    "\n",
    "        # Normalize to have mean 0 and variance 1\n",
    "        norm_X_train = (X_train - train_mean) / train_stddev\n",
    "        norm_X_val = (X_val - train_mean) / train_stddev\n",
    "        norm_X_test = (X_test - train_mean) / train_stddev\n",
    "\n",
    "        pred_train = new_predict_proba(X_train)[:, true_index]\n",
    "        pred_val = new_predict_proba(X_val)[:, true_index]\n",
    "\n",
    "        maple_explainer = MAPLE(norm_X_train, pred_train, norm_X_val, pred_val)\n",
    "\n",
    "        action_set = ActionSet(X = X_train)\n",
    "#         action_set = ActionSet(X = ohe_X_train, custom_bounds={'race_1':(0, 1.0), 'race_4':(0, 1.0)})\n",
    "        display(action_set)\n",
    "\n",
    "        start_time = time.time()\n",
    "        num_neg_test_preds = 0\n",
    "\n",
    "        negative_scores = np.nonzero(scores < 0.5)[0]\n",
    "        recourses = [None] * len(negative_scores)\n",
    "\n",
    "        num_neg_test_preds = len(negative_scores)\n",
    "\n",
    "        columns = X_train.columns\n",
    "        \n",
    "        print(len(X_train))\n",
    "        \n",
    "        if \"cluster\" in explanation_type:\n",
    "            cluster_model = PiecewiseRegressor(verbose=False,\n",
    "                                   binner=DecisionTreeRegressor(max_leaf_nodes = 100))\n",
    "            if explanation_type == \"cluster_train\":\n",
    "                cluster_model.fit(norm_X_train, pred_train)\n",
    "#             else: NEED SAMPLED POINTS\n",
    "#                 cluster_model.fit(lime_sampled, model_preds_sampled, sample_weight=lime_sampled_weights)\n",
    "            cluster_estimators = cluster_model.estimators_\n",
    "            print(len(cluster_estimators))\n",
    "            bins = cluster_model.transform_bins(norm_X_train.values)\n",
    "        \n",
    "        for idx, i in enumerate(negative_scores): #scores is for X_test specifically\n",
    "            if idx % 25 == 0:\n",
    "                print(\"\\n\", idx, \" out of \", len(negative_scores))\n",
    "            if idx % 100 == 0:\n",
    "                print(\"time elapsed: \", (time.time() - start_time) / 60, \" minutes\")\n",
    "                start_time = time.time()\n",
    "            \n",
    "            x = X_test.values[i]\n",
    "\n",
    "            num_features = len(x)\n",
    "\n",
    "            print(explanation_type)\n",
    "            \n",
    "            if \"cluster\" in explanation_type:\n",
    "                normalized_x = (x - train_mean) / train_stddev\n",
    "                estimator_idx = int(cluster_model.transform_bins(np.array([normalized_x]))[0])\n",
    "                print(estimator_idx)\n",
    "                coefs = cluster_estimators[estimator_idx].coef_\n",
    "                intercept = cluster_estimators[estimator_idx].intercept_\n",
    "                \n",
    "                w = coefs / train_stddev\n",
    "                b = intercept - np.sum(w * train_mean) - 0.5 # subtract 0.5 bc using probs as labels                   w, b = \n",
    "                returned_actions, error = get_recourse(x, action_set, w, b)                \n",
    "            \n",
    "            elif explanation_type == \"lime\":\n",
    "                w, b, local_pred, accuracy_sampled = get_lime_coefficients(lime_explainer, x, new_predict_proba, num_features, num_samples, X_train.columns, true_index)\n",
    "                \n",
    "                returned_actions, error = get_recourse(x, action_set, w, b)\n",
    "                \n",
    "            elif explanation_type == \"maple\":\n",
    "                if use_lime_sampled_maple:\n",
    "                    inverse_lime_sampled, scaled_binary_lime_sampled, model_preds_sampled, lime_sampled_weights, mean, std = get_lime_sampled(lime_explainer, x, new_predict_proba, num_features, num_samples, X_train.columns, true_index)\n",
    "#                     un_ohe_lime_sampled = un_ohe(lime_sampled, categorical_names, enc, columns = columns)\n",
    "                    if use_lime_inverse:\n",
    "                        lime_sampled = inverse_lime_sampled\n",
    "                    else:\n",
    "                        lime_sampled = scaled_binary_lime_sampled\n",
    "                    w, b, local_pred, accuracy_sampled = get_maple_coefficients(maple_explainer, x, train_mean, train_stddev, lime_sampled = lime_sampled, model_preds_sampled = model_preds_sampled, use_distance_weights = maple_use_distance_weights)\n",
    "                else:\n",
    "                    w, b, local_pred, accuracy_sampled = get_maple_coefficients(maple_explainer, x, train_mean, train_stddev, use_distance_weights = maple_use_distance_weights)\n",
    "                \n",
    "                returned_actions, error = get_recourse(x, action_set, w, b)\n",
    "                \n",
    "            elif explanation_type == \"piecewise\" or explanation_type == \"piecewise_maple\":\n",
    "                inverse_lime_sampled, scaled_binary_lime_sampled, model_preds_sampled, lime_sampled_weights, mean, std = get_lime_sampled(lime_explainer, x, new_predict_proba, num_features, num_samples, X_train.columns, true_index)\n",
    "                if use_lime_inverse:\n",
    "                    lime_sampled = inverse_lime_sampled\n",
    "                else:\n",
    "                    lime_sampled = scaled_binary_lime_sampled\n",
    "                if explanation_type == \"piecewise\":\n",
    "                    estimators = get_piecewise_coefficients(x, lime_sampled, model_preds_sampled, lime_sampled_weights)\n",
    "                else:\n",
    "                    estimators = get_piecewise_coefficients_with_maple(maple_explainer, x, lime_sampled = lime_sampled, model_preds_sampled = model_preds_sampled, use_distance_weights = False)\n",
    "                returned_actions, error = [], []\n",
    "                \n",
    "                print(\"NUM ESTIMATORS: \", len(estimators))\n",
    "                \n",
    "                for estimator in estimators:\n",
    "                    \n",
    "                    coefs = estimator.coef_\n",
    "                    intercept = estimator.intercept_\n",
    "            \n",
    "                    print(coefs)\n",
    "                    print(intercept)\n",
    "            \n",
    "                    w = coefs / std\n",
    "                    b = intercept - np.sum(w * mean) - 0.5 # subtract 0.5 bc using probs as labels                    \n",
    "                    \n",
    "                    \n",
    "                    ra, er = get_recourse(x, action_set, w, b)\n",
    "                    for a in ra:\n",
    "                        if not any((a == e).all() for e in returned_actions):\n",
    "                            returned_actions.append(a)\n",
    "                    error.extend([er])\n",
    "                \n",
    "            model_pred = (new_predict_proba([x])[0][true_index])\n",
    "\n",
    "                \n",
    "            recourse = {}\n",
    "            recourse['idx'] = i\n",
    "            recourse['instance'] = x\n",
    "            recourse['model_prob'] = model_pred\n",
    "#             recourse['local_prob'] = local_pred\n",
    "            recourse['model_pred'] = 1 if model_pred >= 0.5 else -1\n",
    "#             recourse['local_pred'] = 1 if local_pred >= 0.5 else -1\n",
    "\n",
    "            recourse['scaled_coeff'] = w\n",
    "            recourse['scaled_intercept'] = b\n",
    "            recourse['actions'] = returned_actions\n",
    "            recourse['error_solving'] = error\n",
    "\n",
    "            recourse['explanation_type'] = explanation_type\n",
    "\n",
    "#             recourse['accurate_pred'] = 1 if (recourse['model_pred'] == recourse['local_pred']) else 0\n",
    "#             recourse['sampled_accuracy'] = accuracy_sampled\n",
    "\n",
    "            recourse['returned_actions'] = returned_actions\n",
    "    \n",
    "            recourses[idx] = recourse\n",
    "\n",
    "            print_coefs = False\n",
    "\n",
    "                                    \n",
    "            no_changes = True\n",
    "            \n",
    "            if len(returned_actions) != 0:\n",
    "                total_instances_with_recourses += 1\n",
    "                instances_with_recourses.append(i) \n",
    "            \n",
    "            for action in returned_actions:\n",
    "                new_x = (x + action)\n",
    "                ohe_new_x = ohe(new_x.reshape(1, -1), categorical_names, enc, columns = columns)\n",
    "                \n",
    "                \n",
    "                old_pred = recourse['model_pred']\n",
    "                new_pred = model.predict(ohe_new_x)[0]\n",
    "\n",
    "                new_lime_pred = 1 if np.dot(w, new_x) + b >= 0.0 else -1\n",
    "                total_recourses += 1\n",
    "                \n",
    "                if i in instances_subset and calculate_subset_accuracy:\n",
    "                    subset_total_recourses += 1\n",
    "                \n",
    "                if old_pred != new_pred:\n",
    "                    print(get_nonzero_actions(columns, action))\n",
    "                    total_actual_recourses += 1\n",
    "                    no_changes = False\n",
    "                    \n",
    "                    if i in instances_subset and calculate_subset_accuracy:\n",
    "                        subset_total_actual_recourses +=1\n",
    "                \n",
    "            if no_changes:\n",
    "                print(x)\n",
    "                    \n",
    "            print(\"model_pred: \", recourse['model_pred']) \n",
    "#             print(\"local_pred: \", recourse['local_pred'])\n",
    "            print(\"intercept: \", b)\n",
    "\n",
    "        if explanation_type == \"piecewise\":\n",
    "            total_errors = [1 for rec in recourses if (True not in rec['error_solving'])]            \n",
    "        else:\n",
    "            total_errors = [1 for rec in recourses if (rec['error_solving'] == True)]   \n",
    "#         total_accurate_preds = [1 for rec in recourses if (rec['accurate_pred'] == True)]   \n",
    "#         average_sampled_accuracy = np.mean([rec['sampled_accuracy'] for rec in recourses])\n",
    "        average_recourses_per_all = np.mean([len(rec['returned_actions']) for rec in recourses])\n",
    "        average_recourses_per_found = np.mean([len(rec['returned_actions']) for rec in recourses if rec['returned_actions'] != []])\n",
    "\n",
    "        try:\n",
    "        \n",
    "            print(\"num_neg_test_preds: \", num_neg_test_preds, \" out of \", len(scores), \" = \", round(num_neg_test_preds/len(scores), 2), file=f)\n",
    "            print(\"recourse accuracy: \", round(total_actual_recourses/total_recourses, 2), \"; total instances with recourses found: \", total_instances_with_recourses, file=f)\n",
    "            print(\"recourse accuracy (on all instances, assuming recourse and assuming avg per found instance would be found): \", round(total_actual_recourses/(average_recourses_per_found * num_neg_test_preds), 2), file=f)\n",
    "            if instances_subset != []:\n",
    "                print(\"subset recourse accuracy: \", round(subset_total_actual_recourses/subset_total_recourses, 2), \"; total instances in subset: \", len(instances_subset), file=f)\n",
    "                print(\"subset recourse accuracy (out of total potential): \", round(subset_total_actual_recourses/(len(instances_subset) * 20), 2), \"; total instances in subset: \", len(instances_subset), file=f)\n",
    "            print(\"number of errors: \", sum(total_errors), \"; percent of total instances: \", round(sum(total_errors)/len(recourses), 2), file=f)\n",
    "            print(\"average number of recourses per instance: \", round(average_recourses_per_all, 2), file=f)\n",
    "            print(\"average number of recourses per instance found: \", round(average_recourses_per_found, 2), file=f)\n",
    "#             print(\"number accurate preds (original data): \", sum(total_accurate_preds), \"; percent of total instances: \", round(sum(total_accurate_preds)/len(recourses), 2), file=f)\n",
    "#             print(\"average accuracy of preds on sampled data: \", average_sampled_accuracy, file=f)\n",
    "\n",
    "        except ZeroDivisionError as error_msg:\n",
    "            print(error_msg)\n",
    "            \n",
    "        return instances_with_recourses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X, y, test_size = 0.5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_size)\n",
    "    \n",
    "    data = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    return data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lime_base' from '../lime_experiments/lime_base.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sys\n",
    "import os\n",
    "# import lime.explanation\n",
    "# import lime.lime_tabular\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(\"../lime_experiments\")\n",
    "    \n",
    "import explanation\n",
    "import lime_tabular\n",
    "import lime_base\n",
    "\n",
    "import importlib\n",
    "importlib.reload(explanation)\n",
    "importlib.reload(lime_tabular)\n",
    "importlib.reload(lime_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(file_name, model, enc, data, categorical_features, categorical_names):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    exp1 = {'explanation_type': 'lime', 'use_lime_sampled_maple': None, 'lime_sample_around_instance': False, 'maple_use_distance_weights': None, 'instances_subset': None}\n",
    "\n",
    "    experiments = [exp1]\n",
    "\n",
    "    for exp in experiments:\n",
    "        lime_instances = calculate_recourse_accuracy(model, data, enc, categorical_features, categorical_names, \\\n",
    "                                    file_name, explanation_type = exp[\"explanation_type\"], \\\n",
    "                                    use_lime_sampled_maple = exp['use_lime_sampled_maple'], \\\n",
    "                                    lime_sample_around_instance = exp['lime_sample_around_instance'], \\\n",
    "                                    maple_use_distance_weights = exp['maple_use_distance_weights'], instances_subset = exp['instances_subset'])\n",
    "\n",
    "    print(\"TIME FOR EXP1: \", (time.time() - start_time) / 60, \" minutes\")\n",
    "    cluster_exp = {'explanation_type': 'cluster_train', 'use_lime_sampled_maple': None, 'lime_sample_around_instance': False, 'maple_use_distance_weights': None, 'instances_subset': lime_instances}\n",
    "    \n",
    "    exp2 = {'explanation_type': 'piecewise_maple', 'use_lime_sampled_maple': None, 'lime_sample_around_instance': False, 'maple_use_distance_weights': None, 'instances_subset': lime_instances}\n",
    "    exp3 = {'explanation_type': 'piecewise', 'use_lime_sampled_maple': None, 'lime_sample_around_instance': False, 'maple_use_distance_weights': None, 'instances_subset': lime_instances}\n",
    "\n",
    "    exp4 = {'explanation_type': 'maple', 'use_lime_sampled_maple': True, 'lime_sample_around_instance': False, 'maple_use_distance_weights': True, 'instances_subset': lime_instances}\n",
    "    exp5 = {'explanation_type': 'maple', 'use_lime_sampled_maple': True, 'lime_sample_around_instance': False, 'maple_use_distance_weights': False, 'instances_subset': lime_instances}\n",
    "\n",
    "    exp6 = {'explanation_type': 'maple', 'use_lime_sampled_maple': False, 'lime_sample_around_instance': False, 'maple_use_distance_weights': False, 'instances_subset': lime_instances}\n",
    "    exp7 = {'explanation_type': 'maple', 'use_lime_sampled_maple': False, 'lime_sample_around_instance': False, 'maple_use_distance_weights': True, 'instances_subset': lime_instances}\n",
    "\n",
    "    experiments = [cluster_exp, exp2, exp3, exp4, exp5, exp6, exp7]\n",
    "\n",
    "\n",
    "    for exp in experiments:\n",
    "        _ = calculate_recourse_accuracy(model, data, enc, categorical_features, categorical_names, \\\n",
    "                                    file_name, explanation_type = exp[\"explanation_type\"], \\\n",
    "                                    use_lime_sampled_maple = exp['use_lime_sampled_maple'], \\\n",
    "                                    lime_sample_around_instance = exp['lime_sample_around_instance'], \\\n",
    "                                    maple_use_distance_weights = exp['maple_use_distance_weights'], instances_subset = exp['instances_subset'])\n",
    "\n",
    "    with open(file_name, \"a\") as f:\n",
    "        print(\"--------------------------------------------\", file=f)\n",
    "        print(\"lime_sample_around_instance: TRUE\", file=f)\n",
    "\n",
    "    exp8 = {'explanation_type': 'lime', 'use_lime_sampled_maple': None, 'lime_sample_around_instance': True, 'maple_use_distance_weights': None, 'instances_subset': None}\n",
    "    experiments = [exp7]\n",
    "\n",
    "    for exp in experiments:\n",
    "        lime_instances = calculate_recourse_accuracy(model, data, enc, categorical_features, categorical_names, \\\n",
    "                                    file_name, explanation_type = exp[\"explanation_type\"], \\\n",
    "                                    use_lime_sampled_maple = exp['use_lime_sampled_maple'], \\\n",
    "                                    lime_sample_around_instance = exp['lime_sample_around_instance'], \\\n",
    "                                    maple_use_distance_weights = exp['maple_use_distance_weights'], instances_subset = exp['instances_subset'])\n",
    "\n",
    "    exp9 = {'explanation_type': 'piecewise', 'use_lime_sampled_maple': None, 'lime_sample_around_instance': True, 'maple_use_distance_weights': None, 'instances_subset': lime_instances}\n",
    "    exp10 = {'explanation_type': 'maple', 'use_lime_sampled_maple': True, 'lime_sample_around_instance': True, 'maple_use_distance_weights': True, 'instances_subset': lime_instances}\n",
    "    exp11 = {'explanation_type': 'maple', 'use_lime_sampled_maple': True, 'lime_sample_around_instance': True, 'maple_use_distance_weights': False, 'instances_subset': lime_instances}\n",
    "\n",
    "\n",
    "    experiments = [exp9, exp10, exp11]\n",
    "\n",
    "    for exp in experiments:\n",
    "        _ = calculate_recourse_accuracy(model, data, enc, categorical_features, categorical_names, \\\n",
    "                                    file_name, explanation_type = exp[\"explanation_type\"], \\\n",
    "                                    use_lime_sampled_maple = exp['use_lime_sampled_maple'], \\\n",
    "                                    lime_sample_around_instance = exp['lime_sample_around_instance'], \\\n",
    "                                    maple_use_distance_weights = exp['maple_use_distance_weights'], instances_subset = exp['instances_subset'])\n",
    "\n",
    "    print(\"TOTAL TIME FOR ALL EXPERIMENTS: \", (time.time() - start_time) / 60, \" minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation score:  0.83\n",
      "test predictions split: \n",
      " 1    513\n",
      "-1     57\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lime.explanation\n",
    "import lime.lime_tabular\n",
    "\n",
    "# NEURAL NETWORK\n",
    " \n",
    "compas_nn = MLPClassifier()\n",
    "\n",
    "compas_data = get_data(compas_X, compas_y, test_size = 0.25)\n",
    "\n",
    "ohe_compas_X_train = ohe(compas_data['X_train'], compas_categorical_names, compas_enc)\n",
    "ohe_compas_X_val = ohe(compas_data['X_val'], compas_categorical_names, compas_enc)\n",
    "ohe_compas_X_test = ohe(compas_data['X_test'], compas_categorical_names, compas_enc)\n",
    "\n",
    "compas_nn.fit(ohe_compas_X_train, compas_data['y_train']) \n",
    "print(\"validation score: \", round(compas_nn.score(ohe_compas_X_val, compas_data['y_val']), 2))\n",
    "test_preds = pd.Series(compas_nn.predict(ohe_compas_X_test))\n",
    "print(\"test predictions split: \")\n",
    "print(test_preds.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num unique preds:  2\n",
      " 1    513\n",
      "-1     57\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "+-----------------+---------------+---------+------------+----------------+----------------+-----------+-----------+-----------+------+-------+\n",
       "|            name | variable type | mutable | actionable | step direction | flip direction | grid size | step type | step size |   lb |    ub |\n",
       "+-----------------+---------------+---------+------------+----------------+----------------+-----------+-----------+-----------+------+-------+\n",
       "|             age | <class 'int'> |    True |       True |              0 |            nan |        47 |  relative |      0.01 | 20.0 |  66.0 |\n",
       "|  two_year_recid | <class 'int'> |    True |       True |              0 |            nan |         2 |  relative |      0.01 |  0.0 |   1.0 |\n",
       "| c_charge_degree | <class 'int'> |    True |       True |              0 |            nan |         2 |  relative |      0.01 |  0.0 |   1.0 |\n",
       "|            race | <class 'int'> |    True |       True |              0 |            nan |         6 |  relative |      0.01 |  0.0 |   5.0 |\n",
       "|             sex | <class 'int'> |    True |       True |              0 |            nan |         2 |  relative |      0.01 |  0.0 |   1.0 |\n",
       "|    priors_count | <class 'int'> |    True |       True |              0 |            nan |        23 |  relative |      0.01 |  0.0 |  22.0 |\n",
       "|  length_of_stay | <class 'int'> |    True |       True |              0 |            nan |        83 |  relative |      0.01 | -1.0 | 244.0 |\n",
       "+-----------------+---------------+---------+------------+----------------+----------------+-----------+-----------+-----------+------+-------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:61: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4627\n",
      "\n",
      " 0  out of  57\n",
      "time elapsed:  2.0432472229003907e-05  minutes\n",
      "lime\n",
      "exp.local_exp: [(1, -0.10112168450826348), (3, 0.03829063832407629), (2, 0.03349494759616605), (5, -0.026086645328056168), (0, 0.01658108318955907), (4, 0.00904306173035872), (6, -0.0009401531152060787)]\n",
      "populating...\n",
      "WARNING: Loading a SolverResults object with a warning status into\n",
      "    model=unknown;\n",
      "        message from solver=<undefined>\n",
      "excepting...\n",
      "solver status is not OK\n",
      "[25.  0.  0.  0.  1.  9.  2.]\n",
      "model_pred:  -1\n",
      "intercept:  -0.17620986107001507\n",
      "lime\n",
      "exp.local_exp: [(1, -0.11229163301035912), (3, 0.04229693388560304), (5, -0.025520650116081643), (2, 0.024923005013287407), (0, 0.01584320578521159), (4, -0.004252522719370366), (6, -0.0010155583450584507)]\n",
      "populating...\n",
      "obtained 20 items in 0.7 seconds\n",
      "{'age': 34.0, 'race': 2.0, 'priors_count': -13.0}\n",
      "{'age': 34.0, 'race': 2.0, 'priors_count': -13.0, 'length_of_stay': -1.0}\n",
      "{'age': 1.0, 'two_year_recid': -1.0, 'race': 2.0}\n",
      "{'age': 1.0, 'two_year_recid': -1.0, 'race': 2.0, 'length_of_stay': -1.0}\n",
      "{'two_year_recid': -1.0, 'race': 2.0, 'priors_count': -1.0}\n",
      "{'two_year_recid': -1.0, 'race': 2.0, 'priors_count': -1.0, 'length_of_stay': -1.0}\n",
      "{'age': 1.0, 'two_year_recid': -1.0, 'race': 2.0, 'priors_count': -1.0}\n",
      "{'age': 1.0, 'two_year_recid': -1.0, 'race': 2.0, 'priors_count': -1.0, 'length_of_stay': -1.0}\n",
      "{'age': 13.0, 'two_year_recid': -1.0, 'priors_count': -13.0}\n",
      "{'age': 13.0, 'two_year_recid': -1.0, 'priors_count': -13.0, 'length_of_stay': -1.0}\n",
      "{'age': 19.0, 'c_charge_degree': 1.0, 'race': 2.0, 'priors_count': -12.0}\n",
      "{'age': 19.0, 'c_charge_degree': 1.0, 'race': 2.0, 'priors_count': -12.0, 'length_of_stay': -1.0}\n",
      "{'age': 31.0, 'race': 2.0, 'sex': -1.0, 'priors_count': -13.0}\n",
      "{'age': 31.0, 'race': 2.0, 'sex': -1.0, 'priors_count': -13.0, 'length_of_stay': -1.0}\n",
      "{'two_year_recid': -1.0, 'race': 3.0}\n",
      "{'c_charge_degree': 1.0, 'race': 3.0, 'priors_count': -9.0}\n",
      "{'c_charge_degree': 1.0, 'race': 3.0, 'priors_count': -9.0, 'length_of_stay': -1.0}\n",
      "{'two_year_recid': -1.0, 'c_charge_degree': 1.0, 'race': 2.0}\n",
      "{'two_year_recid': -1.0, 'c_charge_degree': 1.0, 'race': 2.0, 'length_of_stay': -1.0}\n",
      "model_pred:  -1\n",
      "intercept:  -0.13780832473801097\n",
      "lime\n",
      "exp.local_exp: [(1, -0.06901286795705719), (3, 0.03482303397723745), (5, -0.029222988168224554), (0, 0.016972463605742224), (2, -0.008594484977254218), (4, 0.00424847395459557), (6, -0.00013546652027897945)]\n",
      "populating...\n",
      "WARNING: Loading a SolverResults object with a warning status into\n",
      "    model=unknown;\n",
      "        message from solver=<undefined>\n",
      "excepting...\n",
      "solver status is not OK\n",
      "[ 32.   1.   0.   0.   1.  17. 209.]\n",
      "model_pred:  -1\n",
      "intercept:  -0.19963734053437054\n",
      "lime\n",
      "exp.local_exp: [(5, -0.04670756631033171), (0, 0.015220672957881346), (2, 0.010817237340152637), (3, 0.00662469883748072), (6, -0.006085271038651002), (1, -0.0015252695246615036), (4, -0.0005655366482632664)]\n",
      "populating...\n",
      "recovered all minimum-cost items\n",
      "obtained 0 items in 0.0 seconds\n",
      "[56.  1.  0.  0.  1. 31.  0.]\n",
      "model_pred:  -1\n",
      "intercept:  0.28669839404540065\n",
      "lime\n",
      "exp.local_exp: [(1, -0.11640263013964794), (3, 0.04143734771239231), (5, -0.026885713676893745), (2, 0.018262597898713234), (0, 0.016440638285946316), (4, 0.006536326045644742), (6, -0.0006441004115404715)]\n",
      "populating...\n"
     ]
    }
   ],
   "source": [
    "run_all(\"3_28.txt\", compas_nn, compas_enc, compas_data, compas_categorical_features, \\\n",
    "        compas_categorical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_exp = {'explanation_type': 'cluster_train', 'use_lime_sampled_maple': None, 'lime_sample_around_instance': False, 'maple_use_distance_weights': None, 'instances_subset': None}\n",
    "\n",
    "_ = calculate_recourse_accuracy(compas_nn, compas_data, compas_enc, compas_categorical_features, compas_categorical_names, \\\n",
    "                                \"cluster_train.txt\", explanation_type = cluster_exp[\"explanation_type\"], \\\n",
    "                                use_lime_sampled_maple = cluster_exp['use_lime_sampled_maple'], \\\n",
    "                                lime_sample_around_instance = cluster_exp['lime_sample_around_instance'], \\\n",
    "                                maple_use_distance_weights = cluster_exp['maple_use_distance_weights'], instances_subset = cluster_exp['instances_subset'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "compas_rf = RandomForestClassifier()\n",
    "\n",
    "compas_rf.fit(ohe_compas_X_train, compas_data['y_train']) \n",
    "print(\"validation score: \", round(compas_rf.score(ohe_compas_X_val, compas_data['y_val']), 2))\n",
    "test_preds = pd.Series(compas_rf.predict(ohe_compas_X_test))\n",
    "print(\"test predictions split: \")\n",
    "print(test_preds.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(\"3_27_rf_V2.txt\", compas_rf, compas_enc, compas_data, compas_categorical_features, \\\n",
    "        compas_categorical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NETWORK\n",
    "\n",
    "german_nn = MLPClassifier()\n",
    "\n",
    "german_data = get_data(german_X, german_y)\n",
    "\n",
    "german_nn.fit(german_data['X_train'], german_data['y_train']) \n",
    "print(\"validation score: \", round(german_nn.score(german_data['X_val'], german_data['y_val']), 2))\n",
    "test_preds = pd.Series(german_nn.predict(german_data['X_test']))\n",
    "print(\"test predictions split: \")\n",
    "print(test_preds.value_counts())\n",
    "\n",
    "# german_rf = RandomForestClassifier(n_estimators=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(\"3_23_german_nn.txt\", german_nn, german_data, german_categorical_features, \\\n",
    "        german_categorical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# NEURAL NETWORK\n",
    "\n",
    "german_rf = RandomForestClassifier()\n",
    "\n",
    "german_rf.fit(german_data['X_train'], german_data['y_train']) \n",
    "print(\"validation score: \", round(german_rf.score(german_data['X_val'], german_data['y_val']), 2))\n",
    "test_preds = pd.Series(german_rf.predict(german_data['X_test']))\n",
    "print(\"test predictions split: \")\n",
    "print(test_preds.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(\"3_23_german_rf.txt\", german_rf, german_data, german_categorical_features, \\\n",
    "        german_categorical_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NETWORK\n",
    "\n",
    "adult_nn = MLPClassifier()\n",
    "\n",
    "adult_data = get_data(adult_X, adult_y, test_size = 0.1)\n",
    "\n",
    "adult_nn.fit(adult_data['X_train'], adult_data['y_train']) \n",
    "print(\"validation score: \", round(adult_nn.score(adult_data['X_val'], adult_data['y_val']), 2))\n",
    "test_preds = pd.Series(adult_nn.predict(adult_data['X_test']))\n",
    "print(\"test predictions split: \")\n",
    "print(test_preds.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(\"3_23_adult_nn.txt\", adult_nn, adult_data, adult_categorical_features, \\\n",
    "        adult_categorical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "adult_rf = RandomForestClassifier()\n",
    "\n",
    "adult_rf.fit(adult_data['X_train'], adult_data['y_train']) \n",
    "print(\"validation score: \", round(adult_rf.score(adult_data['X_val'], adult_data['y_val']), 2))\n",
    "test_preds = pd.Series(adult_rf.predict(adult_data['X_test']))\n",
    "print(\"test predictions split: \")\n",
    "print(test_preds.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(\"3_23_adult_rf.txt\", adult_rf, adult_data, adult_categorical_features, \\\n",
    "        adult_categorical_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can switch optimizers if you don't have CPLEX by setting `optimizer=\"cbc\"`. \n",
    "\n",
    "A quick note: Our decision boundary is by default 0. We shift this by tweaking the intercept. Since we used Logistic Regression, we use the trick above to do that. In future iterations, we will provide a more elegant way of doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = rb.fit()\n",
    "output_1\n",
    "\n",
    "all_info = rb.populate()\n",
    "print(all_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great, we have a solution! This individual has recourse. The total cost of all the actions needed to flip their prediction is the first thing of interest to us. It costs this person $.21$, meaning that the sum of percentile shifts across this person's features is $.21$. That's quite a lot. Imagine having to shift that much relative to a population? Let's check out what this means in terms of actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(output_1['actions'], index=X.columns).to_frame('Actions')\n",
    "actions = [x['actions'] for x in all_info]\n",
    "actions_df = pd.DataFrame(data=actions).transpose().set_index(X.columns)\n",
    "person = (pd.Series(x, index=X.columns))\n",
    "print(person)\n",
    "display(actions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let's read this. \n",
    "\n",
    "* `SavingsAccountBalance_geq_100`$=1$, for example. This was a binary feature, so it can only be $1$. This also means that we're enouraging this person to increase their savings. \n",
    "* `LoanDuration`$=20$. This, if we recall, was the number of months of loan. This means we're encouraging this person to reapply but specify that their loan repayment period is 20 months shorter.\n",
    "\n",
    "Let's check if these two actions make sense in the context of this person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[denied_individuals[0]].to_frame(\"Original Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this person originally applied with no savings and with a 4-year repayment period. So asking them to get savings and decrease their loan repayment period by $20$ months make sense as actions.\n",
    "\n",
    "(Let's leave aside the question of mutually exclusive features (eg. `SavingsAccountBalance_geq_100` $=0$, `SavingsAccountBalance_geq_500`$=1$). We'll get back to that in later releases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's close by noting some things:\n",
    "\n",
    "* Immutable features are __not__ changed. That's good. That's recourse.\n",
    "* The changes make sense, at least directionally. We'd encourage this person to get a gaurantor, to decrease their loan amount, and to decrease their loan period, among other changes.\n",
    "\n",
    "Yes, these might be hard for someone. They might have other reasons for immutability that we're not considering. Maybe they _need_ that amount and cannot change. Ok, let's express that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_set['LoanAmount'].mutable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.values[denied_individuals[0]]\n",
    "\n",
    "p = .8\n",
    "rb = RecourseBuilder(\n",
    "      optimizer=\"cbc\",\n",
    "      coefficients=coefficients,\n",
    "      intercept=intercept- (np.log(p / (1. - p))),\n",
    "      action_set=action_set,\n",
    "      x=x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2 = rb.fit()\n",
    "output_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so their total cost actually didn't change, which is nice. Let's take a look at their new action set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(output_2['actions'], index=X.columns).to_frame(\"New Actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, by decreasing their repayment period by a bit more and changing some other features, this person can still ask for the same amount. That's good.\n",
    "\n",
    "The magical thing about both of these action sets is that this person, if they do this, _will_ qualify for a loan. Let's check that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba([X.loc[denied_individuals[0]] + pd.Series(output_1['actions'], index=X.columns)])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba([X.loc[denied_individuals[0]] + pd.Series(output_2['actions'], index=X.columns)])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it. By making these tweaks, this person has two ways to get over the $.8$ threshold that we've set. This period can now get approved under this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
